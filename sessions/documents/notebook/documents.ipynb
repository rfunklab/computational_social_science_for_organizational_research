{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational Social Science for Organizational Research\n",
    "## Documents and Text\n",
    "\n",
    "## Roadmap\n",
    " * Corpora\n",
    " * Documents as bags of words\n",
    " * Tokenizing\n",
    " * Stemming and lemmatization\n",
    " * Zipf's law\n",
    " * Cleaning out the bag\n",
    " * Document similarity\n",
    " * Tf-Idf weighting\n",
    " * Exercises\n",
    " \n",
    "Natural language processing and related techniques represent, in my view, some of the most exciting areas of computational social science, especially for organizational researchers. As we all no doubt know from personal experience, organizations are great at generating massive amounts of documents. While that's always been true, digitization has taken it to a whole new level. Documents are the digital footprints of organizational activity. By studying documents, we can gain unprecedented insight into how organizations work. \n",
    "\n",
    "The challenge, however, is that much of the data we care about is highly unstructured, in the form of written text. Although traditional qualitative methods are valuable, the sheer scale of documents means that we can only read and evaluate a small subset of them by hand. You will also have trouble loading a text document into a rectangular matrix in your favorite statistical software.\n",
    "\n",
    "In this session, you'll get a crash course in how to start analyzing documents and text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpora\n",
    "\n",
    "Most typically, we'll use techniques from natural language processing when we have a large collection of documents that we would like to model or from which we would like to extract data or insights. In the land of natural language processing, a collection (or \"body\") of documents is called a [corpus](https://en.wikipedia.org/wiki/Text_corpus) (plural *corpora*). You can think of a corpus as your raw data. Most often, you'll probably be collecting a corpus of documents from some organization or research setting of interest, but there are also many publicly available corpora (often used for benchmarking new algorithms), like those from the [Linguistic Data Consortium](https://catalog.ldc.upenn.edu/).\n",
    "\n",
    "To help make our explorations of natural language processing more concrete, let's put together a simple corpus. In particular, let's download the abstracts for all patents that are classified as \"nanotechnology\" (B82Y) by the Cooperative Patent Classificaion scheme. We can do so easily using the code below, which queries the Patents View Application Programming Interface (API). Don't worry if you don't follow all the API wrangling, which is a bit beyond the scope of what we want to cover today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load some packages\n",
    "import requests\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "\n",
    "# set some configuration parameters\n",
    "DATE_START = \"1997-01-01\"\n",
    "DATE_END = \"2003-12-31\"\n",
    "PER_PAGE = 9999\n",
    "BASE_URL = \"https://www.patentsview.org/api/patents/query\"\n",
    "CPC_GROUP_ID = \"B82Y\"\n",
    "\n",
    "\"\"\"\n",
    "# format our api query\n",
    "params = {'q': '{\"_and\":[{\"_gte\":{\"patent_date\":\"%s\"}},{\"_lt\":{\"patent_date\":\"%s\"}},{\"cpc_group_id\":\"%s\"}]}' % (DATE_START, \n",
    "                                                                                                                DATE_END, \n",
    "                                                                                                                CPC_GROUP_ID),\n",
    "          'f': '[\"patent_number\",\"patent_date\",\"patent_abstract\"]',\n",
    "          'o': '{\"per_page\":%s}' % (PER_PAGE,) }\n",
    "\n",
    "# send the query to the server\n",
    "r = requests.get(BASE_URL, params=params)\n",
    "\n",
    "# save response as dict\n",
    "abstracts_df = pd.DataFrame(r.json()[\"patents\"]).set_index(\"patent_number\")\n",
    "\n",
    "# make a directory to store the data\n",
    "pathlib.Path(\"pickles\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# save as pickle\n",
    "abstracts_df.to_pickle(\"pickles/abstracts_df.pickle\")\n",
    "\"\"\"\n",
    "\n",
    "# read from pickle\n",
    "abstracts_df = pd.read_pickle(\"pickles/abstracts_df.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patent_date</th>\n",
       "      <th>patent_abstract</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>patent_number</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5591238</th>\n",
       "      <td>1997-01-07</td>\n",
       "      <td>A method and nickel-containing catalyst are di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5591487</th>\n",
       "      <td>1997-01-07</td>\n",
       "      <td>An information recording layer on a substrate ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5591532</th>\n",
       "      <td>1997-01-07</td>\n",
       "      <td>A single layer film is deposited onto a substr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5591690</th>\n",
       "      <td>1997-01-07</td>\n",
       "      <td>The present invention relates to a gettering m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5591710</th>\n",
       "      <td>1997-01-07</td>\n",
       "      <td>Compositions and processes to alleviate oxygen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6671141</th>\n",
       "      <td>2003-12-30</td>\n",
       "      <td>A ferromagnetic tunnel effective film has a fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6671301</th>\n",
       "      <td>2003-12-30</td>\n",
       "      <td>A semiconductor laser device including: a semi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6671443</th>\n",
       "      <td>2003-12-30</td>\n",
       "      <td>A method for forming a hybrid active electroni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6671624</th>\n",
       "      <td>2003-12-30</td>\n",
       "      <td>The present invention provides systems, method...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6671625</th>\n",
       "      <td>2003-12-30</td>\n",
       "      <td>A technique is disclosed that is useful for de...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8322 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              patent_date                                    patent_abstract\n",
       "patent_number                                                               \n",
       "5591238        1997-01-07  A method and nickel-containing catalyst are di...\n",
       "5591487        1997-01-07  An information recording layer on a substrate ...\n",
       "5591532        1997-01-07  A single layer film is deposited onto a substr...\n",
       "5591690        1997-01-07  The present invention relates to a gettering m...\n",
       "5591710        1997-01-07  Compositions and processes to alleviate oxygen...\n",
       "...                   ...                                                ...\n",
       "6671141        2003-12-30  A ferromagnetic tunnel effective film has a fr...\n",
       "6671301        2003-12-30  A semiconductor laser device including: a semi...\n",
       "6671443        2003-12-30  A method for forming a hybrid active electroni...\n",
       "6671624        2003-12-30  The present invention provides systems, method...\n",
       "6671625        2003-12-30  A technique is disclosed that is useful for de...\n",
       "\n",
       "[8322 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstracts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documents as bags of words\n",
    "\n",
    "Free-flowing text is too unstructured for computers to hanle. We're going to need to impose some order that will make a document look a bit more like traditional quantitative data (think rectangular matrices). There is a tradeoff here. On the one hand, by imposing structure, we're going to end up throwing away a lot of (important) information that is critical to a full understanding of the textual content of our documents. On the other hand, structure will also allow to use mathematical and statistical techniques, and therefore permit new kinds of insights on our corpus that would otherwise be impossible.\n",
    "\n",
    "There are many different ways we could think about imposing structure on free flowing text. In this session, we're going to use what is known in natural language processing as the \"bag of words\" model. I love the name of the model because it's so descriptive. As you might guess from the name, under this model, we're not going to worry about word order. We can think of as a jumble of words all mixed up in a shopping bag. What we will care about is word frequency. The basic idea is that even if you don't know anything about the word order of a document, knowing the relative frequency of different words tells you some important information. \n",
    "\n",
    "As an example, consider the following two sentences, taken from the abstract of patent [10,563,325](http://patft1.uspto.gov/netacgi/nph-Parser?patentnumber=10563325).\n",
    "\n",
    "> A method for making carbon fiber film includes growing a carbon nanotube array on a surface of a growth substrate.\n",
    "\n",
    "We might represent this in a bag-of-words format as a Python tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = (\"A\", \"method\", \"for\", \"making\", \"carbon\", \"fiber\", \"film\", \"includes\", \"growing\",\n",
    "        \"a\", \"carbon\", \"nanotube\", \"array\", \"on\", \"a\", \"surface\", \"of\", \"a\", \"growth\", \n",
    "        \"substrate.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want a little more structure, we can also represent as a Python dictionary, with keys representing words, and values representing frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_1 = {\"a\": 3, \"carbon\": 2, \"A\": 1, \"method\": 1, \"for\": 1, \"making\": 1, \"fiber\": 1,\n",
    "              \"film\": 1, \"includes\": 1, \"growing\": 1, \"nanotube\": 1, \"array\": 1, \"on\": 1,\n",
    "              \"surface\": 1, \"of\": 1, \"growth\": 1, \"substrate.\": 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dictionary representation is nice, because it helps us to see that what we're essentially doing is treating documents as vectors of words. Note that we can easily extend this approach to multiple documents. Imagine that we encountered another nanotechnology patent that had the following sentence in its abstract.\n",
    "\n",
    "> A method for making a carbon nanotube.\n",
    "\n",
    "We can represent that patent using a dictionary just like we did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_2 = {\"A\": 1, \"method\": 1, \"for\": 1, \"making\": 1, \"a\": 1, \"carbon\": 1, \"nanotube.\": 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And from there, it's not a big leap to think that we might represent both documents together in a matrix.\n",
    "\n",
    "| term       | document_1 | document_2 |\n",
    "|------------|------------|------------|\n",
    "| a          | 3          | 1          |\n",
    "| A          | 1          | 1          |\n",
    "| array      | 1          | 0          |\n",
    "| carbon     | 2          | 1          |\n",
    "| fiber      | 1          | 0          |\n",
    "| film       | 1          | 0          |\n",
    "| for        | 1          | 1          |\n",
    "| growing    | 1          | 0          |\n",
    "| growth     | 1          | 0          |\n",
    "| includes   | 1          | 0          |\n",
    "| making     | 1          | 0          |\n",
    "| method     | 1          | 1          |\n",
    "| nanotube   | 1          | 1          |\n",
    "| of         | 1          | 0          |\n",
    "| on         | 1          | 0          |\n",
    "| substrate. | 1          | 0          |\n",
    "| surface    | 1          | 0          |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing\n",
    "\n",
    "In the example above, we took some short snippets of text from two patents and represented them as vectors (or \"bags of words\"), where elements represent counts of words. Implicitly, then, we split each sentence into its constituent words, or tokens. For anyone who's learned how to read, that's a pretty straightforward task; something we can do quite effortlessly. But it turns out for a computer, splitting a text into its constitutent words can be quite a challenge, at least if you want quality results. In natural language processing, this task is called __tokenization__. \n",
    "\n",
    "Probably the simplest way to tokenize a sentence is to split on white space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'love',\n",
       " 'visiting',\n",
       " 'New',\n",
       " 'York.',\n",
       " \"It's\",\n",
       " 'so',\n",
       " 'much',\n",
       " 'fun---I',\n",
       " 'visit',\n",
       " 'whenever',\n",
       " 'I',\n",
       " 'can!']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"I love visiting New York. It's so much fun---I visit whenever I can!\"\n",
    "s.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad, but there are a few problems. \"New York\" was split into two words, which, although correct in some sense, does not correspond to the fact that in this sentence, \"New\" and \"York\" are functioning as a single unit. In addition, notice that \"York\" still has a period attached at the end, and \"can\" has an exclamation point. Finally, our technique treats \"fun---I\" as a single word.\n",
    "\n",
    "Instead of splitting on white space, we can do a little better by splitting on word boundaries (e.g., punctuation, white space). In Python, we can do that easily using a regular expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'I',\n",
       " ' ',\n",
       " 'love',\n",
       " ' ',\n",
       " 'visiting',\n",
       " ' ',\n",
       " 'New',\n",
       " ' ',\n",
       " 'York',\n",
       " '. ',\n",
       " 'It',\n",
       " \"'\",\n",
       " 's',\n",
       " ' ',\n",
       " 'so',\n",
       " ' ',\n",
       " 'much',\n",
       " ' ',\n",
       " 'fun',\n",
       " '---',\n",
       " 'I',\n",
       " ' ',\n",
       " 'visit',\n",
       " ' ',\n",
       " 'whenever',\n",
       " ' ',\n",
       " 'I',\n",
       " ' ',\n",
       " 'can',\n",
       " '!']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "re.split(r\"\\b\", s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've made some improvement, like separating words from punctuation. But some problems remain, like \"New York\" being split. We've also introduced some __new__ problems, like \"It's\" now being split into \"It\" and \"'s\". It is possible to do a bit better by using some fancier regular expressions. But you're still going to miss tricky cases like \"New York.\" \n",
    "\n",
    "Although we can spend a lot of time worrying about the quality of our tokenization, it reality, even fairly unsophisticated approaches like the ones we've just explored can produce reasonable results, depending on what you're trying to do. Still, it's not hard to get our hands on a more sophisticated tokenizer. \n",
    "\n",
    "We'll be using a package called spaCy for most of this session. Let's see whether its tokenizer improves on our previous attempts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'love',\n",
       " 'visiting',\n",
       " 'New',\n",
       " 'York',\n",
       " '.',\n",
       " 'It',\n",
       " \"'s\",\n",
       " 'so',\n",
       " 'much',\n",
       " 'fun',\n",
       " '---',\n",
       " 'I',\n",
       " 'visit',\n",
       " 'whenever',\n",
       " 'I',\n",
       " 'can',\n",
       " '!']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load spacy\n",
    "import spacy\n",
    "\n",
    "# load model\n",
    "nlp = spacy.load(\"en_core_web_lg\") \n",
    "\n",
    "# run nlp on our sentence\n",
    "[token.text for token in nlp(s)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not really! Still, for most use cases, an industrial strength tokenizer (like spaCy) is going to perform better than our word-boundary approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming and lemmatization\n",
    "\n",
    "Let's take a look at a vector representation of the tokens from our favorite sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'I': 3,\n",
       "         'love': 1,\n",
       "         'visiting': 1,\n",
       "         'New': 1,\n",
       "         'York': 1,\n",
       "         '.': 1,\n",
       "         'It': 1,\n",
       "         \"'s\": 1,\n",
       "         'so': 1,\n",
       "         'much': 1,\n",
       "         'fun': 1,\n",
       "         '---': 1,\n",
       "         'visit': 1,\n",
       "         'whenever': 1,\n",
       "         'can': 1,\n",
       "         '!': 1})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load collections\n",
    "import collections\n",
    "\n",
    "# make a vector of counts\n",
    "collections.Counter([token.text for token in nlp(s)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that our vector treats \"visit\" and \"visiting\" as two distinct words. Of course, these morphological differences are gramatically important in written texts, but in our bag of words model, we've done away with grammar. So for our purposes, we might want to treat \"visit\" and \"visiting\" as the same word. We can do so by applying two related techniques, stemming and lemmatization.\n",
    "\n",
    "As the name suggests, stemming entails reducing words to their \"stems\" by removing tenses, plurals, and related morphological information. So, for example, here is an example of various forms of the word \"participate\" taken to their stems.\n",
    "\n",
    "| word          | stem     |\n",
    "|---------------|----------|\n",
    "| participate   | particip |\n",
    "| participating | particip |\n",
    "| participated  | particip |\n",
    "| participation | particip |\n",
    "| participates  | particip |\n",
    "\n",
    "Lemmatization is similar to stemming, but rather groups together common words by their \"canonical\" or \"dictionary\" form. The upshot is that while stems are not necessarily themselves actual words, lemmas are. To round out the example, here are the lemmas for our variations on \"participate.\"\n",
    "\n",
    "| word          | stem          |\n",
    "|---------------|---------------|\n",
    "| participate   | participate   |\n",
    "| participating | participate   |\n",
    "| participated  | participate   |\n",
    "| participation | participation |\n",
    "| participates  | participate   |\n",
    "\n",
    "As you might have guessed, we can get lemmas easily in spaCy. Let's go back to our nanotechnology example, and pull the lemmas for the abstract for patent 5531930. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 2),\n",
       " ('-', 2),\n",
       " ('contain', 2),\n",
       " ('compound', 2),\n",
       " ('method', 1),\n",
       " ('and', 1),\n",
       " ('nickel', 1),\n",
       " ('catalyst', 1),\n",
       " ('be', 1),\n",
       " ('disclose', 1),\n",
       " ('for', 1),\n",
       " ('prepare', 1),\n",
       " ('synthesis', 1),\n",
       " ('gas', 1),\n",
       " ('by', 1)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patent_5591238_abstract = abstracts_df.loc[\"5591238\"].patent_abstract\n",
    "patent_5591238_lemmas = [token.lemma_ for token in nlp(patent_5591238_abstract)]\n",
    "patent_5591238_lemmas = collections.Counter(patent_5591238_lemmas)\n",
    "patent_5591238_lemmas.most_common(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zipf's law\n",
    "\n",
    "Although it may not seem like much, we can already start doing some interesting analyses using the tools we've learned so far. Let's try a quick applications to get a better feel for what we can do...and to learn a bit more about our nanotechnology patents. \n",
    "\n",
    "Some words are much more common than others. Words like \"the,\" \"and,\" and \"are\" appear many times per document, while words like \"hurlyburly,\" \"piquant,\" and \"vituperate\" might appear only a few times in an entire corpus (though exactly which words are common or rare depends on the content of the corpus). If we plot a frequency distribution of words, we'll notice that they follow a pattern such that the frequency of a word is roughly inversely proportional to its rank, i.e.,\n",
    "\n",
    "$$ n \\propto \\frac{1}{r^\\alpha}$$,\n",
    "\n",
    "where $n$ is the expected frequency, $r$ is the rank, and $\\alpha$ is a scalining parameter, typically estimated to be close to 1. This phenomenon is known as Zipf's Law. Let's investigate Zipf's Law using some of the tools we've just learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list to hold tokens\n",
    "tokens = []\n",
    "\n",
    "# loop over abstracts and pull tokens\n",
    "for patent_abstract in abstracts_df.patent_abstract:\n",
    "  tokens.extend([token.text for token in nlp(patent_abstract, disable=[\"ner\",\"parser\"])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a vector of counts\n",
    "token_counts = collections.Counter(tokens)\n",
    "\n",
    "# make a series\n",
    "tokens_s = pd.Series(token_counts).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's plot the result on a log-log plot. We'll be hoping to see a more or less straight line running from the top left corner of the plot to the bottom right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x17ccc1040>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# make a log-log plot\n",
    "tokens_s.plot(loglog=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning out the bag\n",
    "\n",
    "So far, we've gone from natural language text to a bag of words. Our main trick for doing so was to split words into discrete tokens and to throw them into some sort of list or vector. We also learned that we could reduce some noise in our bag of words by collapsing different forms into a common stem or lemma.\n",
    "\n",
    "But out bag of words is still quite messy. Let's take a look at some of the most common things we'll see when we reach inside. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 63092),\n",
       " ('a', 48567),\n",
       " ('of', 39551),\n",
       " (',', 36676),\n",
       " ('.', 31833),\n",
       " ('and', 28026),\n",
       " ('to', 19977),\n",
       " ('is', 16836),\n",
       " ('-', 15624),\n",
       " ('layer', 14111),\n",
       " ('in', 13745),\n",
       " ('The', 11660),\n",
       " ('an', 10927),\n",
       " ('for', 9233),\n",
       " (')', 7999)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_counts.most_common(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few things stand out. First, some of the most common tokens in our bag are not words at all, but punctuation. We also have the same word being treated as two different tokens based on capitalization (\"The\" and \"the\"). Typically, before you start doing any serious textual analysis, you'll want to clean up your bag of words a bit. These steps, usually called preprocessing, typically entail things like removing punctuation, changing all tokens to a common case (e.g., all lower case), lemmatization, and so forth. The particular steps you end up taking will depend a lot on the nature of your data. So let's clean up our patent abstracts a little. We'll hold our preprocessed tokens in a new column of our data frame. We'll start by defining a function that will do the preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set some configuration parameters\n",
    "MIN_TOKEN_LENGTH = 2\n",
    "MAX_TOKEN_LENGTH = 50\n",
    "\n",
    "# define our function\n",
    "def preprocess_patent_abstract(patent_abstract):\n",
    "  \"\"\"preprocess a patent abstract\"\"\"\n",
    "  \n",
    "  # create a list to save the preprocessed tokens\n",
    "  patent_abstract_pp = []\n",
    "    \n",
    "  # tokenize\n",
    "  patent_abstract_nlp = nlp.tokenizer(patent_abstract)\n",
    "    \n",
    "  # loop over the tokens and keep only what we want\n",
    "  for token in patent_abstract_nlp:\n",
    "    if (token.is_stop is False \n",
    "        and token.is_punct is False \n",
    "        and token.is_digit is False \n",
    "        and token.is_space is False\n",
    "        and len(token) > MIN_TOKEN_LENGTH\n",
    "        and len(token) < MAX_TOKEN_LENGTH):\n",
    "        patent_abstract_pp.append(token.lemma_.lower())    \n",
    "    \n",
    "  # return our cleaned up tokens\n",
    "  return patent_abstract_pp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patent_date</th>\n",
       "      <th>patent_abstract</th>\n",
       "      <th>patent_abstract_pp</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>patent_number</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5591238</th>\n",
       "      <td>1997-01-07</td>\n",
       "      <td>A method and nickel-containing catalyst are di...</td>\n",
       "      <td>[method, nickel, contain, catalyst, disclose, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5591487</th>\n",
       "      <td>1997-01-07</td>\n",
       "      <td>An information recording layer on a substrate ...</td>\n",
       "      <td>[information, record, layer, substrate, provid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5591532</th>\n",
       "      <td>1997-01-07</td>\n",
       "      <td>A single layer film is deposited onto a substr...</td>\n",
       "      <td>[single, layer, film, deposit, substrate, room...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5591690</th>\n",
       "      <td>1997-01-07</td>\n",
       "      <td>The present invention relates to a gettering m...</td>\n",
       "      <td>[present, invention, relate, gettering, materi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5591710</th>\n",
       "      <td>1997-01-07</td>\n",
       "      <td>Compositions and processes to alleviate oxygen...</td>\n",
       "      <td>[compositions, process, alleviate, oxygen, tox...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              patent_date                                    patent_abstract  \\\n",
       "patent_number                                                                  \n",
       "5591238        1997-01-07  A method and nickel-containing catalyst are di...   \n",
       "5591487        1997-01-07  An information recording layer on a substrate ...   \n",
       "5591532        1997-01-07  A single layer film is deposited onto a substr...   \n",
       "5591690        1997-01-07  The present invention relates to a gettering m...   \n",
       "5591710        1997-01-07  Compositions and processes to alleviate oxygen...   \n",
       "\n",
       "                                              patent_abstract_pp  \n",
       "patent_number                                                     \n",
       "5591238        [method, nickel, contain, catalyst, disclose, ...  \n",
       "5591487        [information, record, layer, substrate, provid...  \n",
       "5591532        [single, layer, film, deposit, substrate, room...  \n",
       "5591690        [present, invention, relate, gettering, materi...  \n",
       "5591710        [compositions, process, alleviate, oxygen, tox...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstracts_df[\"patent_abstract_pp\"] = abstracts_df.patent_abstract.apply(preprocess_patent_abstract)\n",
    "abstracts_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feels good to be clean!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document similarity\n",
    "\n",
    "When we're working with textual data, one of the most common things we'll want to do is make comparisons among documents based on how similar or different they are. The canonical example of this comes from information retrieval, where we might be given a string (e.g., as a query from a user of a search engine) and we want to find the set of documents that are most similar. Returning to our running example for this session, we might be interested in knowing how similar or different a particular nanotechnology patent is from other nanotechnology patents, under the idea that more distinctive patents might represent more radical innovations.\n",
    "\n",
    "We already have many of the tools we need to start comparing documents. As I noted a while back, the beauty of representing our documents as vectors is that we can treat them like mathematical objects. Rather than inventing new techniques for comparing documents based on their textual similarity, we can use standard distance metrics, of which there are many. In natural language processing, the most widely used is cosine distance (similarity), which characterizes distance between vectors based on their angle of separation in a vector space. Cosine distance has several nice properties. When the vector elements are positive integers (which is what we'll have for our document vectors), the measure ranges from 0 to 1, which makes interpretation easy. In addition, unlike other metrics (e.g., Euclidean distance), cosine distance is not sensitive to document size. Let's take a quick look at a few examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc1 vs. doc2 euclidean: 14.832396974191326\n",
      "doc1 vs. doc2 cosine: 0.0\n",
      "doc3 vs. doc4 euclidean: 12.806248474865697\n",
      "doc3 vs. doc4 cosine: 0.495475020890487\n"
     ]
    }
   ],
   "source": [
    "# load scipy distance library\n",
    "import scipy.spatial.distance\n",
    "\n",
    "# create some dummy documents\n",
    "doc1 = [2,4,6,8,10]\n",
    "doc2 = [4,8,12,16,20]\n",
    "doc3 = [2,4,6,8,10]\n",
    "doc4 = [2,4,6,0,0]\n",
    "\n",
    "# doc1, doc2\n",
    "print(\"doc1 vs. doc2 euclidean: {}\".format(scipy.spatial.distance.euclidean(doc1, doc2)))\n",
    "print(\"doc1 vs. doc2 cosine: {}\".format(scipy.spatial.distance.cosine(doc1, doc2)))\n",
    "\n",
    "# doc3, doc4\n",
    "print(\"doc3 vs. doc4 euclidean: {}\".format(scipy.spatial.distance.euclidean(doc3, doc4)))\n",
    "print(\"doc3 vs. doc4 cosine: {}\".format(scipy.spatial.distance.cosine(doc3, doc4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's compute the cosine similarity for some real abstracts. First, we'll need to get a vector representation for each abstract we want to compare. We can do this using the methods we learned above. The trick here is that we need to make sure the two vectors we want to compare all contain the same elements. That means, for example, that if the word \"nanotube\" appears in abstract A but not abstract B, we still need to represent \"nanotube\" in vector B, but with a count of 0. In addition, we need to make sure that the vectors are aligned by dimension, such that the words are in the same order. Here is a little function that will take two lists of tokens and compute their cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our function\n",
    "def cosine_similarity_from_token_lists(tl1, tl2):\n",
    "  \"\"\"return cosine similarity from two token lists.\"\"\"\n",
    "  \n",
    "  # make sure we have data in both lists\n",
    "  if len(tl1) < 1 or len(tl2) < 1:\n",
    "    return None\n",
    "\n",
    "  # create a list with the common tokens\n",
    "  common_tokens = list(set(tl1 + tl2))\n",
    "\n",
    "  # create lists to hold counts for tl1 and tl2\n",
    "  tl1_counts = []\n",
    "  tl2_counts = []\n",
    "\n",
    "  # loop over common_tokens and fill lists of counts\n",
    "  for common_token in common_tokens:\n",
    "    \n",
    "    # add count of common token to tl1_counts\n",
    "    tl1_counts.append(tl1.count(common_token))\n",
    "    \n",
    "    # add count of common token to tl1_counts\n",
    "    tl2_counts.append(tl2.count(common_token))\n",
    "  \n",
    "  # compute cosine similarity\n",
    "  cosine_similarity = 1.0 - scipy.spatial.distance.cosine(tl1_counts, tl2_counts)  \n",
    "  \n",
    "  # return the result\n",
    "  return cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's nice about defining a function is that we can write the code once and use it over and over again. And, if we want to make a change later on, we just need to edit in one place. Now that we've got our function, let's run some comparisons. The code below will select two random patents and give their textual similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03209824257403826"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# randomly select a first abstract\n",
    "patent_abstract_1 = abstracts_df.sample(1).iloc[0].patent_abstract_pp\n",
    "\n",
    "# randomly select a second abstract\n",
    "patent_abstract_2 = abstracts_df.sample(1).iloc[0].patent_abstract_pp\n",
    "\n",
    "# get cosine similarity\n",
    "cosine_similarity_from_token_lists(patent_abstract_1, patent_abstract_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final example, let's compare the similarity of nanotechnology patents across years. Perhaps that will give us a sense for technological change in the area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished comparisons for 1997!\n",
      "finished comparisons for 1998!\n",
      "finished comparisons for 1999!\n",
      "finished comparisons for 2000!\n",
      "finished comparisons for 2001!\n",
      "finished comparisons for 2002!\n",
      "finished comparisons for 2003!\n"
     ]
    }
   ],
   "source": [
    "# create a variable to hold years\n",
    "abstracts_df[\"year\"] = pd.DatetimeIndex(abstracts_df.patent_date).year\n",
    "\n",
    "# pull a random sample to speed up computation\n",
    "abstracts_df_sample = abstracts_df.sample(2500)\n",
    "\n",
    "# create a list to save the results\n",
    "results = []\n",
    "\n",
    "# loop over unique years\n",
    "for year_i in abstracts_df[\"year\"].unique():\n",
    "  \n",
    "  # save the individual comparisons in a list\n",
    "  results_year = []\n",
    "\n",
    "  # pull the tokens for year_i\n",
    "  year_i_tokens = [token for document in abstracts_df_sample.loc[abstracts_df[\"year\"] == year_i].patent_abstract_pp \n",
    "                         for token in document]\n",
    "\n",
    "  # pull the tokens for year_j\n",
    "  for year_j in abstracts_df[\"year\"].unique():\n",
    "   \n",
    "    # pull the tokens for year_j\n",
    "    year_j_tokens = [token for document in abstracts_df_sample.loc[abstracts_df[\"year\"] == year_j].patent_abstract_pp \n",
    "                           for token in document]\n",
    "    \n",
    "    # append our comparison\n",
    "    results_year.append(cosine_similarity_from_token_lists(year_i_tokens, year_j_tokens))\n",
    "  \n",
    "  # save the results from the year to the overall list\n",
    "  print(\"finished comparisons for {}!\".format(year_i))\n",
    "  results.append(results_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see what the results look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x18293cdc0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load a few libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# convert our result to a numpy array\n",
    "results = np.array(results)\n",
    "\n",
    "# plot using matplotlib\n",
    "plt.matshow(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tf-Idf weighting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our previous example, we looked at the textual similarity of nanotechnology patents across years. Looking at the heat map, we see higher similarity among adjacent years and lower similarity among years that are more distant. At the same time, if you look at the actual similarities, you'll notice they're all quite high (e.g., in the 0.8s and 0.9s). There may be good substantive reasons for seeing such high similarities (e.g., maybe nanotechnology doesn't change that much from year to year, which certainly seems plausible). At the same time, some features of how we've defined our bag of words may be contributing to the high similarities we observe. \n",
    "\n",
    "In particular, consider that there are some terms that are likely to appear almost all nanotechnology patents (and with high frequency), e.g., \"nanotechnology,\" \"carbon,\" \"method,\" \"device.\" Because these terms are so common, they don't really tell us much about how similar or different two patents are, or how similar or different patents are across years. \n",
    "\n",
    "There are several techniques to deal with these challenges in natural language processing. One that we've already implemented is the removal of stop words (e.g., \"the\", \"is\", \"and\"). Another technique (often used on conjunction with stop word removal) is term weighting. Specifically, we would like to give some terms greater weight than others. \n",
    "\n",
    "One of the most common metrics for term weighting is called term frequency-inverse document frequency weighting, or TF-IDF for short. The idea is that we want to weight terms more highly when they appear more frequently within a particular document and less highly when they appear more commonly across documents.\n",
    "\n",
    "The code below will run through the steps of TF-IDF weighting. Our first step is to get document frequencies for each token. We can make use of some of our previous work here, specifically our cleaned set of tokens, held in `abstracts_df[\"patent_abstract_pp\"]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary to hold our frequencies\n",
    "token_document_freqs = collections.Counter()\n",
    "\n",
    "# loop over abstract tokens and get document frequencies\n",
    "for patent_abstract_tokens in abstracts_df[\"patent_abstract_pp\"]:\n",
    "  token_document_freqs.update(set(patent_abstract_tokens))\n",
    "\n",
    "# check out our list of document frequencies\n",
    "token_document_freqs.most_common(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now, we have document frequencies. Typically, though, we will want *inverse* document frequencies. We'll also usually want to log transform our weights. So let's do a last little bit of processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "disclose         3.179715\n",
       "hydrocarbyl      8.417220\n",
       "catalyst         5.247534\n",
       "compound         3.500163\n",
       "synthesis        4.920712\n",
       "                  ...    \n",
       "ohgr;cm         10.026658\n",
       "1&times;1010    10.026658\n",
       "evarnescent     10.026658\n",
       "stochastic      10.026658\n",
       "obscure         10.026658\n",
       "Length: 15077, dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a dictionary to hold our inverse frequencies\n",
    "token_inverse_document_freqs = {}\n",
    "\n",
    "# loop over tokens to get inverse document frequencies\n",
    "for token, document_freq in token_document_freqs.items():\n",
    "  token_inverse_document_freqs[token] = np.log(len(abstracts_df)/document_freq) + 1\n",
    "\n",
    "# check out some document frequencies\n",
    "pd.Series(token_inverse_document_freqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've already written a function to compute cosine similarity, but with the way we've currently written the function, we cannot make use of tf-idf weights. Let's make some adjustments to allow us to incorporate weights for our terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our function\n",
    "def cosine_similarity_tfidf_weighted_from_token_lists(tl1, tl2, token_inverse_document_freqs=None):\n",
    "  \"\"\"return tf-idf weighted cosine similarity from two token lists.\"\"\"\n",
    "  \n",
    "  # make sure we have data in both lists\n",
    "  if len(tl1) < 1 or len(tl2) < 1:\n",
    "    return None\n",
    "\n",
    "  # create a list with the common tokens\n",
    "  common_tokens = list(set(tl1 + tl2))\n",
    "\n",
    "  # create lists to hold counts for tl1 and tl2\n",
    "  tl1_counts = []\n",
    "  tl2_counts = []\n",
    "\n",
    "  # loop over common_tokens and fill lists of counts\n",
    "  for common_token in common_tokens:\n",
    "    \n",
    "    # add count of common token to tl1_counts\n",
    "    if token_inverse_document_freqs is not None:\n",
    "      tl1_counts.append(tl1.count(common_token) * token_inverse_document_freqs[common_token])\n",
    "    else:\n",
    "      tl1_counts.append(tl1.count(common_token))\n",
    "    \n",
    "    # add count of common token to tl1_counts\n",
    "    if token_inverse_document_freqs is not None:\n",
    "      tl2_counts.append(tl2.count(common_token) * token_inverse_document_freqs[common_token])\n",
    "    else:\n",
    "      tl2_counts.append(tl2.count(common_token))\n",
    "  \n",
    "  # compute cosine similarity\n",
    "  cosine_similarity = 1.0 - scipy.spatial.distance.cosine(tl1_counts, tl2_counts)  \n",
    "  \n",
    "  # return the result\n",
    "  return cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's give it a whirl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05081667940277612"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# randomly select a first abstract\n",
    "patent_abstract_1 = abstracts_df.sample(1).iloc[0].patent_abstract_pp\n",
    "\n",
    "# randomly select a second abstract\n",
    "patent_abstract_2 = abstracts_df.sample(1).iloc[0].patent_abstract_pp\n",
    "\n",
    "# get cosine similarity\n",
    "cosine_similarity_tfidf_weighted_from_token_lists(patent_abstract_1, \n",
    "                                                  patent_abstract_2,\n",
    "                                                  token_inverse_document_freqs=token_inverse_document_freqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  * Update code used to generate the heat map with similarity among years to use tf-idf weights. How do the results change from the unweighted map?\n",
    "  * Write some code that will compute the average similarity between a focal patent and another set of patens. Can you identify any unique features of patents with high versus low similarity to others in their fields?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envpyc_kernel",
   "language": "python",
   "name": "envpyc_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
